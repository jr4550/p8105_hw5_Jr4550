---
title: "p8105_hw5_JR4550"
author: "Jeff Remo"
date: "2025-11-15"
output: github_document
---
## set up environment 
```{r, message = FALSE}
library(tidyverse)
library(rvest)
library(broom)
library(purrr)
```

## Problem 2
Create function
```{r}
set.seed(1)

sim_t_test = function(n = 30, mu, sigma = 5) {
  x = rnorm(n, mean = mu, sd = sigma)
    test_result = t.test(x, mu = 0) %>%
    tidy() %>%
    select(estimate, p.value)
  
  test_result
}
```

#### Simulation for mu 
```{r}
sim_results_all = 
  tibble(
    true_mu = c(0:6)
  ) %>%
  mutate(
    results = map(true_mu, ~rerun(5000, sim_t_test(mu = .x)))
  ) %>%
  unnest(results) %>%
  mutate(
    iteration = rep(1:5000, times = 7)
  ) %>%
  unnest(results)

power_results = 
  sim_results_all %>%
  group_by(true_mu) %>%
  summarize(
    power = mean(p.value < 0.05)
  )
```

#### Plot 1 Power Results 
```{r}
power_results %>%
  ggplot(aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power of One-Sample t-Test vs Effect Size",
    x = "True Value of Î¼",
    y = "Power (Proportion of Times Null Rejected)"
  ) +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1))

avg_estimates = 
  sim_results_all %>%
  group_by(true_mu) %>%
  summarize(
    avg_estimate_all = mean(estimate),
    avg_estimate_rejected = mean(estimate[p.value < 0.05])
  )
```
#### Association of Size and Power: 
The plot shows a strong positive link between effect size and power. When Î¼ = 0 (no effect), power is about 0.05, the same as Î± (the Type I error rate). As the effect size gets bigger, power goes up quickly. When Î¼ = 4, power is close to 1, so we almost always find the true effect. This means itâ€™s easier to spot larger effect sizes with the same sample size and variance.



#### Plot 2 All samples
```{r}
p1 = avg_estimates %>%
  ggplot(aes(x = true_mu, y = avg_estimate_all)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Average Estimate of Î¼Ì‚ (All Samples)",
    x = "True Value of Î¼",
    y = "Average Estimate of Î¼Ì‚"
  ) +
  theme_minimal()
```

# Plot 3: Rejected null only
```{r}
p2 = avg_estimates %>%
  ggplot(aes(x = true_mu, y = avg_estimate_rejected)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Average Estimate of Î¼Ì‚ (Rejected Null Only)",
    x = "True Value of Î¼",
    y = "Average Estimate of Î¼Ì‚"
  ) +
  theme_minimal()
```


```{r}
library(patchwork)
p1 / p2

print("Power by True Î¼:")
print(power_results)

print("\nAverage Estimates:")
print(avg_estimates)
```
#### Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡? Why or why not?

No, not always. When considering all samples, the average estimate equals the true Î¼ because Î¼Ì‚ is unbiased. However, among only the samples where the null was rejected, the average estimate is larger than the true Î¼ for small effect sizes. This happens because when power is low, we only reject the null when we get unusually large sample estimates, creating selection bias. As the true effect size increases and power approaches 1, this bias disappears because we reject the null in nearly all samples.


## Problem 3
Load and Describe data 
```{r}
homicide_df =
  read_csv("data/homicide-data.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names()

glimpse(homicide_df)
```
The homicide dataset contains 52,179 observations and 12 variables documenting homicide cases across 50 U.S. cities. Each row represents a single homicide case with a unique identifier (uid). Key variables include the reported date of the incident, victim demographics (name, race, age, and sex), geographic information (city, state, lat, and long), and case dispo indicating whether the case was closed by arrest, closed without arrest, or remains open with no arrest.

```{r}
homicide_summary = 
  homicide_df%>%
  mutate(
    city_state = str_c(city, ", ", state)
  ) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homicide_summary
```

Prop test for Baltimore, MD
```{r}
baltimore_data = 
  homicide_summary %>%
  filter(city_state == "Baltimore, MD")

baltimore_prop_test = 
  prop.test(
    x = baltimore_data %>% pull(unsolved_homicides),
    n = baltimore_data %>% pull(total_homicides)
  )
baltimore_tidy = 
  broom::tidy(baltimore_prop_test)

baltimore_result = 
  baltimore_tidy %>%
  select(estimate, conf.low, conf.high)

baltimore_result
```

Prop test all cities
```{r}
all_cities_test = 
  homicide_summary %>%
  mutate(
    prop_test_result = purrr::map2(
      unsolved_homicides, 
      total_homicides,
      ~prop.test(.x, .y) %>% broom::tidy()
    )
  ) %>%
  unnest(prop_test_result) %>%
  select(city_state, estimate, conf.low, conf.high)

all_cities_test
```

Unsolved Homicides plot 
```{r}
all_cities_test %>%
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides",
    caption = "Error bars represent 95% confidence intervals"
  ) +
  theme_minimal()
```


```{r}
